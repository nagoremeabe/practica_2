{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><span style=\"color:red\"> Practica 2: Análisis de extremos, análisis espectral y filtros</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# módulos necesarios\n",
    "# modulos propios de python\n",
    "import math\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# módulos de lectura y tratamiento de datos\n",
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import netCDF4 as nc\n",
    "import xarray  as xr\n",
    "\n",
    "# estadistica\n",
    "from scipy import stats\n",
    "from scipy import special\n",
    "\n",
    "\n",
    "# librerias de visualización de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "import cartopy.crs       as ccrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Objetivo 1: Análisis de Extremos </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:black'>En esta primera parte de la práctica 2 se pretende analizar algunos índices de extremos de temperatura o precipitación en las regiones definidas empleando bases de datos de temperatura o precipitacion.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carpeta donde tengo los datos\n",
    "folder = 'data/'\n",
    "\n",
    "# abro y leo datos de mínima temperatura diaria\n",
    "temp_min = xr.open_dataset(f'{folder}tmin_pen.nc')\n",
    "temp_max = xr.open_dataset(f'{folder}tmax_pen.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acoto los datos a las latitudes y longitudes correspondientes al norte\n",
    "lats  = [43.8, 42.5]\n",
    "longs = [-6.0, 0.0]\n",
    "\n",
    "# selecciono los datos con los que voy a tratar\n",
    "temp_min = temp_min.sel(lon=slice(longs[0], longs[1]), lat=slice(lats[0], lats[1]),\n",
    "                        Time=slice(datetime(1973, 1, 1), datetime(2015, 1, 1)))\n",
    "temp_max = temp_max.sel(lon=slice(longs[0], longs[1]), lat=slice(lats[0], lats[1]),\n",
    "                        Time=slice(datetime(1973, 1, 1), datetime(2015, 1, 1)))\n",
    "# separamos los datos en dos periodos\n",
    "temp_min_1 = temp_min.sel(Time=slice('1973', '1994'))\n",
    "temp_max_1 = temp_max.sel(Time=slice('1973', '1994'))\n",
    "\n",
    "temp_min_2 = temp_min.sel(Time=slice('1995', '2015'))\n",
    "temp_max_2 = temp_max.sel(Time=slice('1995', '2015'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkblue\"> Cold Nights </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold nights\n",
    "# calculo de 10th percentile para mi región\n",
    "qt_dims = (\"Time\")\n",
    "qt_values = (0.1)\n",
    "\n",
    "# calculo de los cuantiles para cada subset\n",
    "ds_qt_1 = temp_min_1.quantile(qt_values, dim=qt_dims)\n",
    "ds_qt_2 = temp_min_2.quantile(qt_values, dim=qt_dims)\n",
    "\n",
    "# calculo cual va a ser el valor del quantil sup y inferior\n",
    "minimo = np.min((ds_qt_1.min().tn, ds_qt_2.min().tn))\n",
    "maximo = np.max((ds_qt_1.max().tn, ds_qt_2.max().tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el primer periodo calculo del porcentaje de noches frías\n",
    "num_cold_1 = np.sum(np.where(temp_min_1.tn < ds_qt_1.to_array() , 1, 0),axis=0)\n",
    "\n",
    "n = temp_min_1.tn.shape[0]\n",
    "perc_1 = num_cold_1*100/n\n",
    "# elimino la dimension 1\n",
    "perc_1 = np.squeeze(perc_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el segundo periodo calculo del porcentaje de noches frías\n",
    "num_cold_2 = np.sum(np.where(temp_min_2.tn < ds_qt_1.to_array() , 1, 0),axis=0)\n",
    "\n",
    "n = temp_min_2.tn.shape[0]\n",
    "perc_2 = num_cold_2*100/n\n",
    "# elimino la dimension 1\n",
    "perc_2 = np.squeeze(perc_2)\n",
    "\n",
    "# para hacer el grafico\n",
    "mini = np.min((np.min(perc_1), np.min(perc_2)))\n",
    "maxi = np.max((np.max(perc_1), np.max(perc_2)))\n",
    "\n",
    "# latitud y longitud\n",
    "lon = ds_qt_1.lon\n",
    "lat = ds_qt_1.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representamos el porcentaje para cada punto de la malla de datos\n",
    "fig = plt.figure(figsize=(30,10), tight_layout=False) \n",
    "ax1 = fig.add_subplot(211, projection=ccrs.PlateCarree())\n",
    "ax2 = fig.add_subplot(212, projection=ccrs.PlateCarree())\n",
    "cbarticks = np.arange(mini,maxi,1)\n",
    "\n",
    "ax1.coastlines(linewidth = 2)\n",
    "gl=ax1.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    "ax2.coastlines(linewidth = 2)\n",
    "gl = ax2.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    " \n",
    "cmap= 'Blues'\n",
    "unidades= '[%]'\n",
    "\n",
    "im=ax1.contourf(lon, lat, perc_1, cbarticks,cmap = cmap, extend='both', \n",
    "                  vmin = mini, vmax = maxi)\n",
    "\n",
    "im2=ax2.contourf(lon, lat, perc_2, cbarticks,cmap = cmap, extend='both', \n",
    "                   vmin = mini, vmax = maxi)\n",
    "fig.colorbar(im,ax=ax1,shrink=0.8, label=unidades,orientation='vertical')\n",
    "fig.colorbar(im2,ax=ax2,shrink=0.8, label=unidades,orientation='vertical')\n",
    "ax1.set_title('Porcentaje de noches frías en el primer periodo',fontsize=15)\n",
    "ax2.set_title('Porcentaje de noches frías en el segundo periodo',fontsize=15)\n",
    "fig.suptitle('Porcentaje de noches frías definiendo el percentil con el primer periodo',\n",
    "             fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el primer periodo calculo del porcentaje de noches frías\n",
    "num_cold_1 = np.sum(np.where(temp_min_1.tn < ds_qt_2.to_array() , 1, 0), axis=0)\n",
    "\n",
    "n = temp_min_1.tn.shape[0]\n",
    "perc_1 = num_cold_1*100/n\n",
    "# elimino la dimension 1\n",
    "perc_1 = np.squeeze(perc_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el segundo periodo calculo del porcentaje de noches frías\n",
    "num_cold_2 = np.sum(np.where(temp_min_2.tn < ds_qt_2.to_array() , 1, 0),axis=0)\n",
    "\n",
    "n = temp_min_2.tn.shape[0]\n",
    "perc_2 = num_cold_2*100/n\n",
    "# elimino la dimension 1\n",
    "perc_2 = np.squeeze(perc_2)\n",
    "\n",
    "# para hacer el grafico\n",
    "mini = np.min((np.min(perc_1), np.min(perc_2)))\n",
    "maxi = np.max((np.max(perc_1), np.max(perc_2)))\n",
    "\n",
    "# latitud y longitud\n",
    "lon = ds_qt_1.lon\n",
    "lat = ds_qt_1.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representamos el porcentaje para cada punto de la malla de datos\n",
    "fig = plt.figure(figsize=(30,10), tight_layout=False) \n",
    "ax1 = fig.add_subplot(211, projection=ccrs.PlateCarree())\n",
    "ax2 = fig.add_subplot(212, projection=ccrs.PlateCarree())\n",
    "cbarticks = np.arange(mini,maxi,1)\n",
    "\n",
    "ax1.coastlines(linewidth = 2)\n",
    "gl=ax1.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    "ax2.coastlines(linewidth = 2)\n",
    "gl = ax2.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    " \n",
    "cmap= 'Blues'\n",
    "unidades= '[%]'\n",
    "\n",
    "im=ax1.contourf(lon, lat, perc_1, cbarticks,cmap = cmap, extend='both', \n",
    "                  vmin = mini, vmax = maxi)\n",
    "\n",
    "im2=ax2.contourf(lon, lat, perc_2, cbarticks,cmap = cmap, extend='both', \n",
    "                   vmin = mini, vmax = maxi)\n",
    "fig.colorbar(im,ax=ax1,shrink=0.8, label=unidades,orientation='vertical')\n",
    "fig.colorbar(im2,ax=ax2,shrink=0.8, label=unidades,orientation='vertical')\n",
    "ax1.set_title('Porcentaje de noches frías en el primer periodo',fontsize=15)\n",
    "ax2.set_title('Porcentaje de noches frías en el segundo periodo',fontsize=15)\n",
    "fig.suptitle('Porcentaje de noches frías definiendo el percentil con el segundo periodo',\n",
    "             fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkblue\"> Cold Days </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold nights\n",
    "# calculo de 10th percentile para mi región\n",
    "qt_dims = (\"Time\")\n",
    "qt_values = (0.1)\n",
    "\n",
    "# calculo de los cuantiles para cada subset\n",
    "ds_qt_1 = temp_max_1.quantile(qt_values, dim=qt_dims)\n",
    "ds_qt_2 = temp_max_2.quantile(qt_values, dim=qt_dims)\n",
    "\n",
    "# calculo cual va a ser el valor del quantil sup y inferior\n",
    "minimo = np.min((ds_qt_1.min().tx, ds_qt_2.min().tx))\n",
    "maximo = np.max((ds_qt_1.max().tx, ds_qt_2.max().tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el primer periodo calculo del porcentaje de noches frías\n",
    "num_cold_1 = np.sum(np.where(temp_max_1.tx < ds_qt_1.to_array() , 1, 0),axis=0)\n",
    "\n",
    "n = temp_max_1.tx.shape[0]\n",
    "perc_1 = num_cold_1*100/n\n",
    "# elimino la dimension 1\n",
    "perc_1 = np.squeeze(perc_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el segundo periodo calculo del porcentaje de noches frías\n",
    "num_cold_2 = np.sum(np.where(temp_max_2.tx < ds_qt_1.to_array() , 1, 0),axis=0)\n",
    "\n",
    "n = temp_max_2.tx.shape[0]\n",
    "perc_2 = num_cold_2*100/n\n",
    "# elimino la dimension 1\n",
    "perc_2 = np.squeeze(perc_2)\n",
    "\n",
    "# para hacer el grafico\n",
    "mini = np.min((np.min(perc_1), np.min(perc_2)))\n",
    "maxi = np.max((np.max(perc_1), np.max(perc_2)))\n",
    "\n",
    "# latitud y longitud\n",
    "lon = ds_qt_1.lon\n",
    "lat = ds_qt_1.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representamos el porcentaje para cada punto de la malla de datos\n",
    "fig = plt.figure(figsize=(30,10), tight_layout=False) \n",
    "ax1 = fig.add_subplot(211, projection=ccrs.PlateCarree())\n",
    "ax2 = fig.add_subplot(212, projection=ccrs.PlateCarree())\n",
    "cbarticks = np.arange(mini,maxi,1)\n",
    "\n",
    "ax1.coastlines(linewidth = 2)\n",
    "gl=ax1.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    "ax2.coastlines(linewidth = 2)\n",
    "gl = ax2.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    " \n",
    "cmap= 'Blues'\n",
    "unidades= '[%]'\n",
    "\n",
    "im=ax1.contourf(lon, lat, perc_1, cbarticks,cmap = cmap, extend='both', \n",
    "                  vmin = mini, vmax = maxi)\n",
    "\n",
    "im2=ax2.contourf(lon, lat, perc_2, cbarticks,cmap = cmap, extend='both', \n",
    "                   vmin = mini, vmax = maxi)\n",
    "fig.colorbar(im,ax=ax1,shrink=0.8, label=unidades,orientation='vertical')\n",
    "fig.colorbar(im2,ax=ax2,shrink=0.8, label=unidades,orientation='vertical')\n",
    "ax1.set_title('Porcentaje de días fríos en el primer periodo',fontsize=15)\n",
    "ax2.set_title('Porcentaje de días fríos en el segundo periodo',fontsize=15)\n",
    "fig.suptitle('Porcentaje de días fríos definiendo el percentil con el primer periodo',\n",
    "             fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el primer periodo calculo del porcentaje de noches frías\n",
    "num_cold_1 = np.sum(np.where(temp_max_1.tx < ds_qt_2.to_array() , 1, 0), axis=0)\n",
    "\n",
    "n = temp_max_1.tx.shape[0]\n",
    "perc_1 = num_cold_1*100/n\n",
    "# elimino la dimension 1\n",
    "perc_1 = np.squeeze(perc_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el segundo periodo calculo del porcentaje de noches frías\n",
    "num_cold_2 = np.sum(np.where(temp_max_2.tx < ds_qt_2.to_array() , 1, 0),axis=0)\n",
    "\n",
    "n = temp_max_2.tx.shape[0]\n",
    "perc_2 = num_cold_2*100/n\n",
    "# elimino la dimension 1\n",
    "perc_2 = np.squeeze(perc_2)\n",
    "\n",
    "# para hacer el grafico\n",
    "mini = np.min((np.min(perc_1), np.min(perc_2)))\n",
    "maxi = np.max((np.max(perc_1), np.max(perc_2)))\n",
    "\n",
    "# latitud y longitud\n",
    "lon = ds_qt_1.lon\n",
    "lat = ds_qt_1.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representamos el porcentaje para cada punto de la malla de datos\n",
    "fig = plt.figure(figsize=(30,10), tight_layout=False) \n",
    "ax1 = fig.add_subplot(211, projection=ccrs.PlateCarree())\n",
    "ax2 = fig.add_subplot(212, projection=ccrs.PlateCarree())\n",
    "cbarticks = np.arange(mini,maxi,1)\n",
    "\n",
    "ax1.coastlines(linewidth = 2)\n",
    "gl=ax1.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    "ax2.coastlines(linewidth = 2)\n",
    "gl = ax2.gridlines(draw_labels = True)\n",
    "gl.ylabels_right = False\n",
    "gl.xlabels_top   = False\n",
    "\n",
    " \n",
    "cmap= 'Blues'\n",
    "unidades= '[%]'\n",
    "\n",
    "im=ax1.contourf(lon, lat, perc_1, cbarticks,cmap = cmap, extend='both', \n",
    "                  vmin = mini, vmax = maxi)\n",
    "\n",
    "im2=ax2.contourf(lon, lat, perc_2, cbarticks,cmap = cmap, extend='both', \n",
    "                   vmin = mini, vmax = maxi)\n",
    "fig.colorbar(im,ax=ax1,shrink=0.8, label=unidades,orientation='vertical')\n",
    "fig.colorbar(im2,ax=ax2,shrink=0.8, label=unidades,orientation='vertical')\n",
    "ax1.set_title('Porcentaje de días fríos en el primer periodo',fontsize=15)\n",
    "ax2.set_title('Porcentaje de días fríos en el segundo periodo',fontsize=15)\n",
    "fig.suptitle('Porcentaje de días fríos definiendo el percentil con el segundo periodo',\n",
    "             fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkblue\"> Periodo de Retorno </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones Belen\n",
    "def nor(mu,sigma,x):\n",
    "    #Nota: también podrías usar la función norm.pdf(x,k) del módulo stats del paquete scipy\n",
    "    N=1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * sigma**2))\n",
    "    return N\n",
    "\n",
    "def GEV(x,c):\n",
    "    #Función que calcula la función densidad de probabilidad de Generalized Extrem Value normalizada\n",
    "    # con m = 0, s = 1\n",
    "    #nota: también puedes usar g=stats.genextreme.pdf(x,-c)\n",
    "    if ( abs(c) < 0.005 ) :\n",
    "        g = np.exp(-np.exp(-x))*np.exp(-x)\n",
    "    else:\n",
    "        g = ( np.power( 1 + c * x , -1-1/c) ) * np.exp( - np.power(1 + c * x ,-1/c) )\n",
    "    return g\n",
    "\n",
    "def GEVms(x,c,m,s):\n",
    "    #Función que calcula la función de densidad de probabilidad Generalizada Extreme Value sin normalizar\n",
    "    y = ( x - m ) / s\n",
    "    if ( abs(c) < 0.005 ) :\n",
    "        g = ( np.exp(-np.exp(-y))*np.exp(-y) ) / s\n",
    "    else:\n",
    "        g = ( np.power( 1 + c * y , -1-1/c) ) * np.exp( - np.power(1 + c * y ,-1/c) ) / s\n",
    "    return g\n",
    "\n",
    "def GPDs(y,c,s):\n",
    "    #Función que calcula la función de densidad de probabilidad de Pareto Generalizada \n",
    "    if (abs(c)<0.0001):\n",
    "        h = np.exp(-y/s)*(1/s)\n",
    "    else:\n",
    "        h = (1/s) * ( 1 + c * y / s) ** (-1-1/c)\n",
    "    h[y<=0]=0\n",
    "    return h\n",
    "\n",
    "def MLE_GEV(c,m,s,x):\n",
    "    #Función que calcula la función likelihood sobre GEV con \n",
    "    #los parámetros de entrada c,m,s\n",
    "    #y la muestra de datos x\n",
    "    if ( (s < 0) | np.any(1 + c * (x-m)/s ) < 0):\n",
    "        sal=1e7\n",
    "    else:            \n",
    "        if (abs(c) > 0.00001 ):\n",
    "            t = (1 + c * (x-m)/s )**(-1/c)\n",
    "        else :\n",
    "            t = np.exp(-(x-m)/s)\n",
    "        f = - np.log(s) + (c +1)*np.log(t) -t\n",
    "        sal = f.sum()\n",
    "    return -sal\n",
    "\n",
    "\n",
    "def returnLevel(p, c,m,s):\n",
    "    if (abs(c) > 0.00001 ):\n",
    "        zp = m - ( s / c ) * ( 1 - ( - np.log( 1 - p ) ) ** ( - c ))\n",
    "    else :\n",
    "        zp = m - s * np.log( - np.log( 1 - p ) )\n",
    "    return zp\n",
    "\n",
    "\n",
    "def densityPlotGEV(ax,x,c,m,s):\n",
    "    n, bins, dummy = ax.hist(x,density=True,label='Empirica')\n",
    "    x1, x2 = 2*bins[0] - bins[1], 2* bins[-1] - bins[-2]\n",
    "    ax.set_xlim(x1,x2)\n",
    "    x_x = np.linspace(x1,x2,200)\n",
    "    y_y = GEVms(x_x,c,m,s)\n",
    "    ax.plot(x_x,y_y,'r',label='Ajuste')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_ylabel('probabilidad f(z)')\n",
    "    ax.set_xlabel('z')\n",
    "    return\n",
    "\n",
    "def probabilityPlot(ax,pExp,pTeor):\n",
    "    ax.plot([0,1],[0,1])\n",
    "    ax.plot(pExp,pTeor,'o')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel('Empirical')\n",
    "    ax.set_title('Probability plot')\n",
    "    ax.grid()\n",
    "    return\n",
    "\n",
    "def quantilePlot(ax,xTeor,xSort):\n",
    "    ax.plot([xSort[0],xSort[-1]],[xSort[0],xSort[-1]])\n",
    "    ax.plot(xTeor,xSort,'o')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel('Empirical')\n",
    "    ax.set_title('Quantile plot')\n",
    "    ax.grid()\n",
    "    return\n",
    "\n",
    "def returnPlot(ax,p,zp,pExp,xSort):\n",
    "    ax.semilogx(1/p,zp)\n",
    "    ax.grid()\n",
    "    ax.set_xlim(1e-1,1e3)\n",
    "    ax.semilogx(1/pExp,xSort,'o')\n",
    "    ax.set_title('Return level plot')\n",
    "    ax.set_xlabel('Return period (years)')\n",
    "    ax.set_ylabel('Return level')\n",
    "    return\n",
    "    \n",
    "def diagnosticsGEV(x,c,m,s):\n",
    "    y = (x-m)/s\n",
    "    ySort = np.sort(y)\n",
    "    pExp = np.arange(1,ySort.size+1,1)/(ySort.size+1)\n",
    "    pTeor = stats.genextreme.cdf(ySort,-c)\n",
    "    xSort = np.sort(x)\n",
    "    pExp = np.arange(1,xSort.size+1,1)/(xSort.size+1)\n",
    "    pExpinv = np.arange(xSort.size+1,1,-1)/(xSort.size+1)\n",
    "    xTeor = returnLevel(1-pExp, c,m,s)    \n",
    "    p = np.logspace(-2, -1e-2, 100)\n",
    "    zp=returnLevel(p,c,m,s)\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(14,10))\n",
    "    probabilityPlot(ax[0,0],pExp,pTeor)\n",
    "    quantilePlot(ax[0,1],xTeor,xSort)\n",
    "    returnPlot(ax[1,0],p,zp,pExpinv,xSort)\n",
    "    densityPlotGEV(ax[1,1],x,c,m,s)\n",
    "    fig.suptitle('Diagnostics',weight='bold',fontsize=18)\n",
    "    return\n",
    "\n",
    "def diagnosticsGPD(x,c,m,s,numYears):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(14,10))\n",
    "    fig.suptitle('Diagnostics',weight='bold',fontsize=18)\n",
    "    xSort = np.sort(x)\n",
    "    pExp = np.arange(1,xSort.size+1,1)/(xSort.size+1)\n",
    "    pExpinv = np.arange(xSort.size+1,1,-1)/(xSort.size+1)\n",
    "    rv = stats.genpareto(c,m,s)\n",
    "    pTeor = rv.cdf(xSort)\n",
    "    xTeor = rv.ppf(pExp)\n",
    "    fac = x.size / numYears\n",
    "    p = np.logspace(-4, 2 , 100)\n",
    "    zp = rv.ppf(1-p)\n",
    "    fac = x.size / numYears\n",
    "    returnPlot(ax[1,0],p*fac,zp,pExpinv*fac,xSort)\n",
    "    probabilityPlot(ax[0,0],pExp,pTeor)\n",
    "    quantilePlot(ax[0,1],xTeor,xSort)\n",
    "    densityPlotGPD(ax[1,1],x,c,m,s)\n",
    "    return\n",
    "\n",
    "def densityPlotGPD(ax,x,c,m,s):\n",
    "    n, bins, dummy = ax.hist(x,density=True,label='Empirica')\n",
    "    x1, x2 = 2*bins[0] - bins[1], 2* bins[-1] - bins[-2]\n",
    "    ax.set_xlim(x1,x2)\n",
    "    x_x = np.linspace(x1,x2,200)\n",
    "    y_y = stats.genpareto.pdf(x_x,c,m,s)\n",
    "    ax.plot(x_x,y_y,'r',linewidth = 4, label='Ajuste')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_ylabel('probabilidad h(x)')\n",
    "    ax.set_xlabel('x')\n",
    "    return\n",
    "  \n",
    "\n",
    "def readDataFrame(stationName):\n",
    "    url = '../datos/isd/isd-history.csv'\n",
    "    c=pd.read_csv(url)\n",
    "\n",
    "    #Seleccionamos los años de inicio y final de lectura\n",
    "    yearInit = 1973\n",
    "    yearEnd = 2020\n",
    "\n",
    "    #Construimos el código que necesitamos para bajar los datos de la estación:\n",
    "    station=str(c.loc[(c['STATION NAME']==stationName)]['USAF'].values[0]) + '-' + \\\n",
    "            str(c.loc[(c['STATION NAME']==stationName)]['WBAN'].values[0])\n",
    "    print('Seleccionada la estacion ' + stationName + ' con código ' + station)\n",
    "\n",
    "    #Definimos las columnas y el tipo de dato que va a tener el DataFrame donde vamos a alojar los datos de la estación\n",
    "    colNames=['year','month','day','hour','temp','tempd','slp','windDir','windSpeed','cloud','pcp1h','pcp6h']\n",
    "    dataType=['int','int','int','int','float','float','int','int','int','int','int','int',]\n",
    "\n",
    "    #Definimos los límites de los caractéres donde están los datos en los archivos que vamos a leer \n",
    "    #(Esta información está en: https://www.ncei.noaa.gov/pub/data/noaa/isd-lite/isd-lite-format.txt)\n",
    "    limits=[(0,4),(5,7),(8,10),(11,13),(14,19),(20,25),(26,31),(32,37),(38,43),(44,49),(50,55),(56,61)]\n",
    "    \n",
    "    #Lectura de los datos:\n",
    "    df=pd.DataFrame(columns=colNames)\n",
    "    for iyr in range(yearInit, yearEnd, 1):\n",
    "        fil='../datos/isd/'+station+'-'+str(iyr)+'.gz'\n",
    "        data= pd.read_fwf(fil,sep=' ',compression='gzip',head=0,\n",
    "                     names=colNames,colspecs=limits)\n",
    "        df = df.append(data)\n",
    "\n",
    "    #Cambio el tipo de dato (porque los lee como objetos str y queremos que sean numéricos)\n",
    "    dictType={ colNames[i] : dataType[i] for i in range(12)}\n",
    "    data = df.astype(dictType,copy=True)\n",
    "    #Los datos missing son -9999 en la base de datos: cambiamos por NaN:\n",
    "    data = data.replace(-9999,np.NaN)\n",
    "    \n",
    "    dataDailyMax = data.groupby(['year','month','day']).max().drop(['hour'],axis=1).reset_index()\n",
    "    dataDailyMax['time'] = pd.to_datetime(dataDailyMax[['year','month','day']],yearfirst=True)\n",
    "    dataDailyMax = dataDailyMax.set_index('time')\n",
    "    dataDailyCum = data.groupby(['year','month','day']).sum().drop(['hour'],axis=1).reset_index()\n",
    "    dataDailyCum['time'] = pd.to_datetime(dataDailyCum[['year','month','day']],yearfirst=True)\n",
    "    dataDailyCum = dataDailyCum.set_index('time')\n",
    "    dataDailyMean = data.groupby(['year','month','day']).max().drop(['hour'],axis=1).reset_index()\n",
    "    dataDailyMean['time'] = pd.to_datetime(dataDailyMean[['year','month','day']],yearfirst=True)\n",
    "    dataDailyMean = dataDailyMean.set_index('time')\n",
    "    return dataDailyMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extremos por bloques\n",
    "#===========================================================================================\n",
    "# 1. Elegimos el tamaño de los bloques: de los datasets temp_min, temp_max\n",
    "n = 1 #year\n",
    "\n",
    "# voy a trabajar con pandas\n",
    "df_min = temp_min.to_dataframe()\n",
    "\n",
    "# deshago el multiindice\n",
    "df_min.reset_index(inplace=True)\n",
    "\n",
    "# Separo el dataset por años\n",
    "\n",
    "# elijo como índice la fecha\n",
    "# df_min.set_index('Time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min['year'] = df_min['Time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Objetivo 2: Análisis de las periodicidades fundamentales de los extremos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9553dd649da6d806a91f18686597ba4591a31002d4fb1d1d65e2efff2034f7ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
